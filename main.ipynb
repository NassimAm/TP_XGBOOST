{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation et test du modèle XGBOOST\n",
    "---\n",
    "\n",
    "Dans ce notebook, on va découvrir les différents aspects du modèle XGBOOST. En premier lieu nous allons implémenter le modèle à zéro (from scratch) ensuite en deuxième partie nous allons directement utiliser le modèle depuis l'officielle bibliothèque ([xgboost](https://github.com/dmlc/xgboost)) et effectuer avec différents tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.3', '2.0.1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des modules nécessaires\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.__version__, pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Pour bien débuter, nous posons d'abord les premières notions qu'on va utiliser directement dans les prochaines sections.\n",
    "Tout au long du notebook, en parlant du dataset donné on note le nombre d'individus $n$ et le nombre de caractéristiques de chaque individu $m$. Ou plus explicitement le dataset ($D$) est donné comme il suit:\n",
    "$$D = \\{(x_i,y_i) \\text{ tq } x_i \\in R^m, y_i \\in R\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie servira à la compréhension des algorithmes utilisés dans un modèle XGBOOST en les implémentant de zéro pour les deux cas (Régression et Classification). Pour cela nous allons utiliser seulement la bibliothèque **numpy** qui est utile dans les calculs matriciels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Fonction de prédiction\n",
    "\n",
    "Pour un dataset donnée ($D$), la fonction de prédiction du modèle est la somme des $K + 1$ plus simples fonctions de prédiction. Les fonctions de prédiction ($f_k$) avec $k >= 1$ donne les résultats du parcours des arbres de régression construits lors de l'entrainement du modèle. Pour la fonction initiale ($f_0$), ce n'est qu'une valeur constante initiale déterminée au début de l'entraînement. On donne la fonction de prédiction du modèle donc comme il suit:\n",
    "$$\\hat{y_i} = \\phi(x_i) = f_0 + \\eta \\sum\\limits_{k = 1}\\limits^{K} f_k(x_i)$$\n",
    "Note: $\\eta$ représente le taux d'apprentissage (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.6, 38.2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pred(X, f0, fs, eta=0.3):\n",
    "    return f0 + eta * np.sum(np.array([fk(X) for fk in fs]), axis=0)\n",
    "\n",
    "# Ceci n'est qu'un exemple d'une fonction d'un arbre pour faire fonctionner le test\n",
    "def f_test(X):\n",
    "    return np.sum(np.square(X) + X + 4, axis=1)\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 3],\n",
    "        [4, 5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "f0 = 3.4\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([16.6, 38.2])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "pred(X, f0, [f_test, f_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 Fonction du coût\n",
    "\n",
    "L'étape suivante serait de définir la fonction du coût (loss function) pour évaluer le modèle et le faire converger vers le point optimal. Or on a deux cas: la régression et la classification. Et donc on définira pour chacun des cas sa fonction de coût ensuite leurs dérivées pour pouvoir appliquer le [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) par la suite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.1 Fonction du coût (Régression)\n",
    "\n",
    "Pour la régression, nous allons utiliser la somme des erreurs carrées (sum of squares error, SSE). Elle est définie comme il suit\n",
    "$$SSE = l(y_i,\\hat{y_i}) = \\frac{1}{2} (y_i - \\hat{y_i})^2$$\n",
    "Note: la fraction $\\frac{1}{2}$ est ajoutée juste pour simplifier les calculs (notamment pour la dérivée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02 , 0.125, 0.125])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SSE(Y, Y_pred):\n",
    "    return (1/2) * np.square(Y - Y_pred)\n",
    "\n",
    "Y = np.array([1.2, 3.5, 6.7])\n",
    "Y_pred = np.array([1.0, 4, 7.2])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([0.02 , 0.125, 0.125])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "SSE(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on calcule la dérivée de cette fonction de coût par rapport aux prédictions réalisées. Cela nous permettra de trouver la prédiction qui minimise la fonction coût. La dérivée est donc donnée comme il suit:\n",
    "$$\\frac{\\partial SSE}{\\partial \\hat{y_i}} = \\hat{y_i} - y_i$$\n",
    "Note: La valeur $y_i - \\hat{y_i}$ est appelée le résidu **$r_{ik}$** où $k$ représente le numéro de l'itération. Donc on a:\n",
    "$$r_{ik} = - \\frac{\\partial SSE}{\\partial \\hat{y_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2,  0.5,  0.5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dSSE(Y, Y_pred):\n",
    "    return Y_pred - Y\n",
    "\n",
    "Y = np.array([1.2, 3.5, 6.7])\n",
    "Y_pred = np.array([1.0, 4, 7.2])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([-0.2,  0.5,  0.5])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "dSSE(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2 Fonction du coût (Classification)\n",
    "\n",
    "Pour la classification (dans ce cas binaire), nous allons utiliser l'inverse du log de vraisemblance (negative log-likelihood). Elle est définie comme il suit\n",
    "$$NLL = l(y_i,p_i) = -\\text{ }(\\text{ }y_i \\log(p_i) + (1-y_i) \\log(1-p_i)\\text{ })$$\n",
    "Note: $p_i$ est la probabilité prédite par le modèle. Une valeur proche de 0 signifie que le modèle classe l'individu à la classe négative et une valeur proche de 1 signifie que le modèle classe l'individu à la classe positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35667494, 0.22314355, 0.22314355])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NLL(Y, Y_pred):\n",
    "    return - (Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
    "\n",
    "Y = np.array([1, 0, 1])\n",
    "Y_pred = np.array([0.7, 0.2, 0.8])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([0.35667494, 0.22314355, 0.22314355])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "NLL(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on calcule la dérivée de cette fonction de coût par rapport au log de chance de gain (log of odds). Cela nous permettra de trouver la prédiction qui minimise la fonction coût. On rappele que la chance de gain (odds) d'un évènement de propababilité $p$ est définie par:\n",
    "$$odds = \\frac{p}{1 - p}$$\n",
    "$$\\log(odds) = \\log(\\frac{p}{1 - p})$$\n",
    "Dériver par rapport au $\\log(odds)$ est aussi un choix minutieux pour réduire et faciliter les calculs pour le modèle. Il est permis de faire ce choix car il y a une relation directe entre le $\\log(odds)$ et la probabilité $p$. En effet, On peut récupérer la probalité $p$ depuis $\\log(odds)$ ainsi:\n",
    "$$p = \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}$$\n",
    "Calculons maintenant la dérivée par rapport au $\\log(odds)$:\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -(\\text{ }y_i \\log(p_i) + (1-y_i) \\log(1-p_i)\\text{ })$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(p_i) + y_i \\log(1-p_i) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i (\\log(p_i) - \\log(1-p_i)) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(\\frac{p_i}{1-p_i}) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(odds) - \\log(1-p_i)$$\n",
    "On a:\n",
    "$$\\log(1-p_i) = \\log(1 - \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) = \\log(\\frac{1 + e^{\\log(odds)}}{1 + e^{\\log(odds)}} - \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) = \\log(\\frac{1}{1 + e^{\\log(odds)}})$$\n",
    "$$\\log(1-p_i) = -\\log(1 + e^{\\log(odds)})$$\n",
    "Et donc:\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(odds) + \\log(1 + e^{\\log(odds)})$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = -y_i + \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = p_i - y_i$$\n",
    "Note: La valeur $y_i - p_i$ est aussi appelée le résidu $r_{ik}$ où $k$ représente le numéro de l'itération. Donc on a:\n",
    "$$r_{ik} = - \\frac{\\partial NLL}{\\partial \\log(odds)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3,  0.2, -0.2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dNLL(Y, Y_pred):\n",
    "    return Y_pred - Y\n",
    "\n",
    "Y = np.array([1, 0, 1])\n",
    "Y_pred = np.array([0.7, 0.2, 0.8])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([-0.3,  0.2, -0.2])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "dNLL(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3 Constuction des arbres uniques de régression (Arbres XGBOOST)\n",
    "\n",
    "La fonction de prédiction du modèle XGBOOST dépend des arbres de régression construit lors de l'entrainement. Ces arbres sont des arbres spéciaux appelés des arbres uniques (ou arbres XGBOOST) qui servent à prédir des résidus ($r_{ik}$). En additionnant leurs résultats multipliés par un taux d'apprentissage et ajouté à la prédiction initiale, cela nous donne la prédiction finale du modèle (gradient tree boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.1 Fonction de coût de l'arbre\n",
    "\n",
    "Pendant la construction l'arbre, il faudrait minimiser une certaine fonction de coût (loss function). Son équation est donnée ci-dessous:\n",
    "$$L^{(k)} = \\sum\\limits_{i=1}\\limits^{n} l(y_i,\\hat{y_i}^{(k-1)}+f_k(x_i)) + \\Omega(f_k)$$\n",
    "où\n",
    "- $y_i$ est la sortie réelle pour l'individu $i$\n",
    "- $\\hat{y_i}^{(k)}$ est la prédiction pour l'individu $i$ faite par le modèle à l'itération **$k$**\n",
    "- $f_k$ est la fonction qui donne pour tout individu $x_i$ le résultat de l'arbre construit à l'itération $k$\n",
    "- $l(y_i,\\hat{y_i})$ est la fonction de coût (SSE ou NLL dans notre cas)\n",
    "- $\\Omega(f_k)$ est un terme de régulation qui pénalise la complexité du modèle. Il est donné comme il suit:\n",
    "$$\\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\sum\\limits_{j=1}\\limits^{T} w_j^2$$\n",
    "où\n",
    "- $T$ est le nombre de feuilles dans l'arbre\n",
    "- $w_j$ est le poids de la feuille $j$ dans l'arbre (le calcul de ce poids sera détaillé juste après)\n",
    "- $\\gamma$ est un hyper-paramètre de régularisation, il définit la réduction minimale du coût pour réaliser une partition supplémentaire sur une feuille de l'arbre. Par défaut à 0, une plus grande valeur rend le modèle plus conservatif.\n",
    "- $\\lambda$ est un hyper-paramètre de régularisation L2 sur les poids des feuilles. Par défaut à 1, une plus grande valeur rend le modèle plus conservatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5850000000000013, 1.03, 2.615000000000001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction de coût sans le terme de régularisation\n",
    "def Lk_sans_reg(X, Y, Y_pred_past, fk, loss=SSE):\n",
    "    return np.sum(loss(Y, Y_pred_past + fk(X)))\n",
    "\n",
    "# Terme de régularisation\n",
    "def reg(W, reg_gamma=0, T=32, reg_lambda=1):\n",
    "    return reg_gamma * T + (1/2) * reg_lambda * np.sum(np.square(W))\n",
    "\n",
    "# Fonction de coût complète\n",
    "def Lk(X, Y, Y_pred_past, fk, W, loss=SSE, reg_gamma=0, T=32, reg_lambda=1):\n",
    "    return Lk_sans_reg(X, Y, Y_pred_past, fk, loss) + reg(W, reg_gamma, T, reg_lambda)\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 3],\n",
    "        [4, 5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "Y = np.array([2.2, 8.7])\n",
    "Y_pred_past = np.array([1.0, 7.2])\n",
    "\n",
    "# Ceci n'est qu'un exemple d'une fonction d'un arbre pour faire fonctionner le test\n",
    "def f_test(X):\n",
    "    return np.sum(X/5)\n",
    "\n",
    "W = np.array([0.2, 0.3, -0.8, 0.7])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (1.5850000000000013, 1.03, 2.615000000000001)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Lk_sans_reg(X, Y, Y_pred_past, f_test, loss=SSE), reg(W, reg_gamma=0.1, T=4, reg_lambda=1), Lk(X, Y, Y_pred_past, f_test, W, loss=SSE, reg_gamma=0.1, T=4, reg_lambda=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.2 Approximation de la fonction coût de l'arbre\n",
    "\n",
    "Maintenant que la fonction de coût est bien définie, nous pouvons trouver chaque sortie optimale $f_k(x_i)$ qui minimise le plus la fonction de coût. Celà nécessitera le calcul de dérivée de la fonction coût. Pour optimiser les calculs, le modèle XGBOOST approche le fonction coût par son polynôme de Taylor de second degré:\n",
    "$$l(y_i,\\hat{y_i}^{(k-1)}+f_k(x_i)) \\approx l(y_i,\\hat{y_i}^{(k-1)}) + \\frac{\\partial l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)}} f_k(x_i) + \\frac{1}{2} \\frac{\\partial^2 l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)2}} f_k(x_i)^2$$\n",
    "Et donc\n",
    "$$L^{(k)} \\approx \\sum\\limits_{i=1}\\limits^{n} [\\text{ } l(y_i,\\hat{y_i}^{(k-1)}) + \\frac{\\partial l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)}} f_k(x_i) + \\frac{1}{2} \\frac{\\partial^2 l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)2}} f_k(x_i)^2\\text{ }] + \\Omega(f_k)$$\n",
    "$$L^{(k)} \\approx \\sum\\limits_{i=1}\\limits^{n} [\\text{ } l(y_i,\\hat{y_i}^{(k-1)}) + g_i f_k(x_i) + \\frac{1}{2}h_i f_k(x_i)^2\\text{ }] + \\Omega(f_k)$$\n",
    "où\n",
    "- $g_i$ est la dérivée de la fonction coût par rapport à la prédiction précédente ($\\frac{\\partial l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)}}$)\n",
    "- $h_i$ est la seconde dérivée de la fonction coût par rapport à la prédiction précédente ($\\frac{\\partial^2 l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)2}}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.3 Quality Score et Poids des feuilles\n",
    "Définisons plus explicitement la fonction $f_k$, cette fonction nous donne le résultat renvoyé par l'arbre pour un individu avec ses caractéristiques. Ce résultat sera égal au poids d'une des feuilles ($w_j$) de la façon suivante:\n",
    "$$\\hat{y_i}^{(k)} = \\hat{y_i}^{(k-1)} + f_k(x_i)$$\n",
    "$$r_{ik} = y_i - \\hat{y_i}^{(k)}$$\n",
    "$$\\text{Si } r_{ik} \\in I_j \\text{ tel que } I_j \\text{ est l'ensemble des résidus se trouvant dans la feuille } j$$\n",
    "$$\\text{Donc } f_k(x_i) = w_j$$\n",
    "où $w_j$ est le poids de la feuille $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut considérer maintenant une nouvelle fonction de coût approximative pour l'arbre, où on peut omettre le terme $l(y_i,\\hat{y_i}^{(k-1)})$ qui ne représente que le coût de la précédente prédiction qui n'a pas besoin d'être minimisé dans la courante itération. Donc on a maintenant:\n",
    "\n",
    "$$\\tilde{L}^{(k)} = \\sum\\limits_{i=1}\\limits^{n} [\\text{ } g_i f_k(x_i) + \\frac{1}{2}h_i f_k(x_i)^2\\text{ }] + \\gamma T + \\frac{1}{2} \\lambda \\sum\\limits_{j=1}\\limits^{T} w_j^2$$\n",
    "$$\\tilde{L}^{(k)} = \\sum\\limits_{j=1}\\limits^{T} [\\text{ } (\\sum\\limits_{r_{ik} \\in I_j}g_i) w_j + \\frac{1}{2}(\\sum\\limits_{r_{ik} \\in I_j}h_i + \\lambda) w_j^2\\text{ }] + \\gamma T$$\n",
    "En résolvant l'équation $\\frac{\\partial \\tilde{L}^{(k)}}{\\partial w_j} = 0$ On obtient:\n",
    "$$w_j^* = -\\frac{\\sum\\limits_{r_{ik} \\in I_j}g_i}{\\sum\\limits_{r_{ik} \\in I_j}h_i + \\lambda}$$\n",
    "$$\\tilde{L}^{(k)}(w_j^*) = -\\frac{1}{2} \\sum\\limits_{j=1}\\limits^{T} \\frac{(\\sum\\limits_{r_{ik} \\in I_j}g_i)^2}{\\sum\\limits_{r_{ik} \\in I_j}h_i + \\lambda} + \\gamma T$$\n",
    "$$Q_j = - \\tilde{L}^{(k)}(w_j^*)$$\n",
    "où\n",
    "- $w_j^*$ est le poids optimal de la feuille\n",
    "- $\\tilde{L}^{(k)}(w_j^*)$ est le coût minimal pour l'arbre\n",
    "- $Q_j$ est appelée score de qualité ou similarité (Quality or Similarity Score). Puisque elle est inversement proportionelle au coût, le but est de la maximiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a maintenant toutes les informations pour créer un noeud de l'arbre\n",
    "# On définit une classe abstraite pour représenter un noeud en général\n",
    "class Noeud:\n",
    "\n",
    "    nbr = 0\n",
    "    indent = '\\t'\n",
    "\n",
    "    def __init__(self, num: int, quality_score: float, profondeur: int):\n",
    "        self.num = num # le numéro du caractéristique de division dans X\n",
    "        self.qs = quality_score # le quality score pour le noeud\n",
    "        self.pr = profondeur # la profondeur du noeud\n",
    "        self.fils = {} # les fils du noeud\n",
    "        self.res = [] # les résidus trouvés dans le noeud (applicable que pour les feuilles)\n",
    "        self.gs = [] # les dérivés gi de premier ordre du noeud (applicable que pour les feuilles)\n",
    "        self.hs = [] # les dérivés hi de second ordre du noeud (applicable que pour les feuilles)\n",
    "        self.w = 0 # poids du noeud (applicable que pour les feuilles)\n",
    "\n",
    "    # Cette fonction est pour transformer le noeud à un string\n",
    "    def __str__(self): ...\n",
    "\n",
    "    # predire un échantillon\n",
    "    def predire(self, x: list): ...\n",
    "\n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self): ...\n",
    "\n",
    "# On définit un noeud qu'on appelle NoeudCat pour gérer les données qualitatives, cette classe héritera de la classe Noeud\n",
    "class NoeudCat(Noeud):\n",
    "    # Cette fonction est pour transformer le noeud à un string\n",
    "    def __str__(self):\n",
    "        tmp_indent = Noeud.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # s'il n'y a pas de fils, le noeud est terminal ; on imprime la classe\n",
    "        if (len(self.fils)==0):\n",
    "            return tmp_indent + f'W = {self.w}\\n'\n",
    "        \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        res = \"\"\n",
    "        for valeur in self.fils:\n",
    "            res += tmp_indent + 'Si X[' + str(self.num) + '] est \"' + str(valeur) \n",
    "            res += '\" Alors\\n' + str(self.fils[valeur])\n",
    "        return res\n",
    "\n",
    "    # predire un échantillon\n",
    "    def predire(self, x: list[str]):\n",
    "        # Si le noeud est final, il rend son poids\n",
    "        if (len(self.fils)==0):\n",
    "            return self.w\n",
    "        \n",
    "        # Si la valeur de la colonne respective à ce noeud n'appartient pas à l'ensemble des\n",
    "        # valeurs attendues, on rend np.nan\n",
    "        if not(x[self.num] in self.fils.keys()): \n",
    "            return np.nan\n",
    "        \n",
    "        # Sinon, on rend \n",
    "        return self.fils[x[self.num]].predire(x)\n",
    "\n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self):\n",
    "        nid = 'N' + str(Noeud.nbr)\n",
    "        Noeud.nbr += 1\n",
    "        \n",
    "        # Si le noeud est final, \n",
    "        if (len(self.fils)==0):\n",
    "            return nid, nid + '[label=\"' + str(self.w) + '\" shape=ellipse];\\n'\n",
    "        \n",
    "        # Sinon, \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        res  = nid + '[label=\"X[' + str(self.num) + ']\\\\n'\n",
    "        res += 'QS = ' + str(self.qs) + '\"];\\n'\n",
    "        for valeur in self.fils:\n",
    "            vid, code = self.fils[valeur].graphviz()\n",
    "            res += code\n",
    "            res += nid + ' -> ' + vid + ' [label=\"' + valeur + '\"];\\n'\n",
    "        return nid, res\n",
    "    \n",
    "# On définit un noeud qu'on appelle NoeudNum pour gérer les données quantitatives, cette classe héritera de la classe Noeud\n",
    "class NoeudNum(Noeud):\n",
    "    def __init__(self, num: int, quality_score: float, profondeur: int, val: float):\n",
    "        self.num = num # le numéro du caractéristique de division dans X\n",
    "        self.qs = quality_score # le quality score pour le noeud\n",
    "        self.pr = profondeur # la profondeur du noeud\n",
    "        self.val = val # la valeur du noeud\n",
    "        self.fils = {} # les fils du noeud\n",
    "        self.res = [] # les résidus trouvés dans le noeud (applicable que pour les feuilles)\n",
    "        self.gs = [] # les dérivés gi de premier ordre du noeud (applicable que pour les feuilles)\n",
    "        self.hs = [] # les dérivés hi de second ordre du noeud (applicable que pour les feuilles)\n",
    "        self.w = 0 # poids du noeud (applicable que pour les feuilles)\n",
    "\n",
    "    # Cette fonction est pour transformer le noeud à un string\n",
    "    def __str__(self):\n",
    "\n",
    "        tmp_indent = self.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # s'il n'y a pas de fils, le noeud est terminal ; on imprime la classe\n",
    "        if (len(self.fils)==0):\n",
    "            return tmp_indent + f'W = {self.w}\\n'\n",
    "         \n",
    "        prefix = ' < '\n",
    "        suffix = ''\n",
    "        \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS SINON\n",
    "        res  = ''\n",
    "        res += tmp_indent + 'Si X[' + str(self.num) + '] ' + prefix + str(self.val) + suffix \n",
    "        res += ' Alors\\n' + str(self.fils['g'])\n",
    "        res += tmp_indent + 'Sinon\\n' + str(self.fils['d'])\n",
    "        return res\n",
    "\n",
    "    # predire un échantillon\n",
    "    def predire(self, x: list):\n",
    "        # Si le noeud est final, il rend son poids\n",
    "        if (len(self.fils)==0):\n",
    "            return self.w\n",
    "        \n",
    "        # sinon\n",
    "        if x[self.num] < self.val:\n",
    "            return self.fils['g'].predire(x)\n",
    "        return self.fils['d'].predire(x)\n",
    "\n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self):\n",
    "        nid = 'N' + str(Noeud.nbr)\n",
    "        Noeud.nbr += 1\n",
    "        \n",
    "        # Si le noeud est final, \n",
    "        if (len(self.fils)==0):\n",
    "            return nid, nid + '[label=\"' + str(self.w) + '\" shape=ellipse];\\n'\n",
    "        \n",
    "        # Sinon, \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        prefix = '] < '\n",
    "        res = nid + '[label=\"X[' + str(self.num) + prefix + str(self.val) + '\\\\n'\n",
    "        res += 'QS = ' + str(self.qs) + '\"];\\n'\n",
    "        vid_G, code_G = self.fils['g'].graphviz()\n",
    "        vid_D, code_D = self.fils['d'].graphviz()\n",
    "        \n",
    "        res += code_G + code_D\n",
    "        res += nid + ' -> ' + vid_G + ' [label=\"Vrai\"];\\n'\n",
    "        res += nid + ' -> ' + vid_D + ' [label=\"Faux\"];\\n'\n",
    "        return nid, res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
