{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation et test du modèle XGBOOST\n",
    "---\n",
    "\n",
    "Dans ce notebook, on va découvrir les différents aspects du modèle XGBOOST. En premier lieu nous allons implémenter le modèle à zéro (from scratch) ensuite en deuxième partie nous allons directement utiliser le modèle depuis l'officielle bibliothèque ([xgboost](https://github.com/dmlc/xgboost)) et effectuer avec différents tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.3', '2.0.1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des modules nécessaires\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.__version__, pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Pour bien débuter, nous posons d'abord les premières notions qu'on va utiliser directement dans les prochaines sections.\n",
    "Tout au long du notebook, en parlant du dataset donné on note le nombre d'individus **$n$** et le nombre de caractéristiques de chaque individu **$m$**. Ou plus explicitement le dataset (**$D$**) est donné comme il suit:\n",
    "$$D = \\{(x_i,y_i) \\text{ tq } x_i \\in R^m, y_i \\in R\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie servira à la compréhension des algorithmes utilisés dans un modèle XGBOOST en les implémentant de zéro pour les deux cas (Régression et Classification). Pour cela nous allons utiliser seulement la bibliothèque **numpy** qui est utile dans les calculs matriciels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Fonction de prédiction\n",
    "\n",
    "Pour un dataset donnée (**$D$**), la fonction de prédiction du modèle est la somme des $K + 1$ plus simples fonctions de prédiction. Les fonctions de prédiction ($f_k$) avec $k >= 1$ donne les résultats du parcours des arbres de régression construits lors de l'entrainement du modèle. Pour la fonction initiale ($f_0$), ce n'est qu'une valeur constante initiale déterminée au début de l'entraînement. On donne la fonction de prédiction du modèle donc comme il suit:\n",
    "$$\\hat{y_i} = \\phi(x_i) = f_0 + \\eta \\sum\\limits_{k = 1}\\limits^{K} f_k(x_i)$$\n",
    "Note: **$\\eta$** représente le taux d'apprentissage (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.6, 38.2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pred(X, f0, fs, eta=0.3):\n",
    "    return f0 + eta * np.sum(np.array([fk(X) for fk in fs]), axis=0)\n",
    "\n",
    "# Ceci n'est qu'un exemple d'une fonction d'un arbre pour faire fonctionner le test\n",
    "def f_test(X):\n",
    "    return np.sum(np.square(X) + X + 4, axis=1)\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 3],\n",
    "        [4, 5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "f0 = 3.4\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([16.6, 38.2])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "pred(X, f0, [f_test, f_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 Fonction du coût\n",
    "\n",
    "L'étape suivante serait de définir la fonction du coût (loss function) pour évaluer le modèle et le faire converger vers le point optimal. Or on a deux cas: la régression et la classification. Et donc on définira pour chacun des cas sa fonction de coût ensuite leurs dérivées pour pouvoir appliquer le [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) par la suite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.1 Fonction du coût (Régression)\n",
    "\n",
    "Pour la régression, nous allons utiliser la somme des erreurs carrées (sum of squares error, SSE). Elle est définie comme il suit\n",
    "$$SSE = l(y_i,\\hat{y_i}) = \\frac{1}{2} (y_i - \\hat{y_i})^2$$\n",
    "Note: la fraction $\\frac{1}{2}$ est ajoutée juste pour simplifier les calculs (notamment pour la dérivée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02 , 0.125, 0.125])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SSE(Y, Y_pred):\n",
    "    return (1/2) * np.square(Y - Y_pred)\n",
    "\n",
    "Y = np.array([1.2, 3.5, 6.7])\n",
    "Y_pred = np.array([1.0, 4, 7.2])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([0.02 , 0.125, 0.125])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "SSE(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on calcule la dérivée de cette fonction de coût par rapport aux prédictions réalisées. Cela nous permettra de trouver la prédiction qui minimise la fonction coût. La dérivée est donc donnée comme il suit:\n",
    "$$\\frac{\\partial SSE}{\\partial \\hat{y_i}} = \\hat{y_i} - y_i$$\n",
    "Note: La valeur $y_i - \\hat{y_i}$ est appelée le résidu **$r_{ik}$** où $k$ représente le numéro de l'itération. Donc on a:\n",
    "$$r_{ik} = - \\frac{\\partial SSE}{\\partial \\hat{y_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2,  0.5,  0.5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dSSE(Y, Y_pred):\n",
    "    return Y_pred - Y\n",
    "\n",
    "Y = np.array([1.2, 3.5, 6.7])\n",
    "Y_pred = np.array([1.0, 4, 7.2])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([-0.2,  0.5,  0.5])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "dSSE(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2 Fonction du coût (Classification)\n",
    "\n",
    "Pour la classification (dans ce cas binaire), nous allons utiliser l'inverse du log de vraisemblance (negative log-likelihood). Elle est définie comme il suit\n",
    "$$NLL = l(y_i,p_i) = -\\text{ }(\\text{ }y_i \\log(p_i) + (1-y_i) \\log(1-p_i)\\text{ })$$\n",
    "Note: **$p_i$** est la probabilité prédite par le modèle. Une valeur proche de 0 signifie que le modèle classe l'individu à la classe négative et une valeur proche de 1 signifie que le modèle classe l'individu à la classe positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35667494, 0.22314355, 0.22314355])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NLL(Y, Y_pred):\n",
    "    return - (Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
    "\n",
    "Y = np.array([1, 0, 1])\n",
    "Y_pred = np.array([0.7, 0.2, 0.8])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([0.35667494, 0.22314355, 0.22314355])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "NLL(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on calcule la dérivée de cette fonction de coût par rapport au log de chance de gain (log of odds). Cela nous permettra de trouver la prédiction qui minimise la fonction coût. On rappele que la chance de gain (odds) d'un évènement de propababilité $p$ est définie par:\n",
    "$$odds = \\frac{p}{1 - p}$$\n",
    "$$\\log(odds) = \\log(\\frac{p}{1 - p})$$\n",
    "Dériver par rapport au $\\log(odds)$ est aussi un choix minutieux pour réduire et faciliter les calculs pour le modèle. Il est permis de faire ce choix car il y a une relation directe entre le $\\log(odds)$ et la probabilité $p$. En effet, On peut récupérer la probalité $p$ depuis $\\log(odds)$ ainsi:\n",
    "$$p = \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}$$\n",
    "Calculons maintenant la dérivée par rapport au $\\log(odds)$:\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -(\\text{ }y_i \\log(p_i) + (1-y_i) \\log(1-p_i)\\text{ })$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(p_i) + y_i \\log(1-p_i) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i (\\log(p_i) - \\log(1-p_i)) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(\\frac{p_i}{1-p_i}) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(odds) - \\log(1-p_i)$$\n",
    "On a:\n",
    "$$\\log(1-p_i) = \\log(1 - \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) = \\log(\\frac{1 + e^{\\log(odds)}}{1 + e^{\\log(odds)}} - \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) = \\log(\\frac{1}{1 + e^{\\log(odds)}})$$\n",
    "$$\\log(1-p_i) = -\\log(1 + e^{\\log(odds)})$$\n",
    "Et donc:\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(odds) + \\log(1 + e^{\\log(odds)})$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = -y_i + \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = p_i - y_i$$\n",
    "Note: La valeur $y_i - p_i$ est aussi appelée le résidu **$r_{ik}$** où $k$ représente le numéro de l'itération. Donc on a:\n",
    "$$r_{ik} = - \\frac{\\partial NLL}{\\partial \\log(odds)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3,  0.2, -0.2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dNLL(Y, Y_pred):\n",
    "    return Y_pred - Y\n",
    "\n",
    "Y = np.array([1, 0, 1])\n",
    "Y_pred = np.array([0.7, 0.2, 0.8])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([-0.3,  0.2, -0.2])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "dNLL(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3 Constuction des arbres uniques de régression (Arbres XGBOOST)\n",
    "\n",
    "La fonction de prédiction du modèle XGBOOST dépend des arbres de régression construit lors de l'entrainement. Ces arbres sont des arbres spéciaux appelés des arbres uniques (ou arbres XGBOOST) qui servent à prédir des résidus (gradient boosting). En additionnant leurs résultats multipliés par un taux d'apprentissage et ajouté à la prédiction initiale, cela nous donne la prédiction finale du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.1 Fonction de coût de l'arbre\n",
    "\n",
    "Pendant la construction l'arbre, il faudrait minimiser une certaine fonction de coût (loss function). Son équation est donnée ci-dessous:\n",
    "$$L^{(k)} = \\sum\\limits_{i=1}\\limits^{n} l(y_i,\\hat{y_i}^{(k-1)}+f_k(x_i)) + \\Omega(f_k)$$\n",
    "où\n",
    "- **$y_i$** est la sortie réelle pour l'individu $i$\n",
    "- **$\\hat{y_i}^{(k)}$** est la prédiction pour l'individu $i$ faite par le modèle à l'itération **$k$**\n",
    "- **$f_k$** est la fonction qui donne pour tout individu $x_i$ le résultat de l'arbre construit à l'itération $k$\n",
    "- **$l(y_i,\\hat{y_i})$** est la fonction de coût (SSE ou NLL dans notre cas)\n",
    "- **$\\Omega(f_k)$** est un terme de régulation qui pénalise la complexité du modèle. Il est donné comme il suit:\n",
    "$$\\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\sum\\limits_{j=1}\\limits^{T} w_j^2$$\n",
    "où\n",
    "- **$T$** est le nombre de feuilles dans l'arbre\n",
    "- **$w_j$** est le poids de la feuille $j$ dans l'arbre (le calcul de ce poids sera détaillé juste après)\n",
    "- **$\\gamma$** est un hyper-paramètre de régularisation, il définit la réduction minimale du coût pour réaliser une partition supplémentaire sur une feuille de l'arbre. Par défaut à 0, une plus grande valeur rend le modèle plus conservatif.\n",
    "- **$\\lambda$** est un hyper-paramètre de régularisation L2 sur les poids des feuilles. Par défaut à 1, une plus grande valeur rend le modèle plus conservatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5850000000000013, 1.03, 2.615000000000001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction de coût sans le terme de régularisation\n",
    "def Lk_sans_reg(X, Y, Y_pred_past, fk, loss=SSE):\n",
    "    return np.sum(loss(Y, Y_pred_past + fk(X)))\n",
    "\n",
    "# Terme de régularisation\n",
    "def reg(W, reg_gamma=0, T=32, reg_lambda=1):\n",
    "    return reg_gamma * T + (1/2) * reg_lambda * np.sum(np.square(W))\n",
    "\n",
    "# Fonction de coût complète\n",
    "def Lk(X, Y, Y_pred_past, fk, W, loss=SSE, reg_gamma=0, T=32, reg_lambda=1):\n",
    "    return Lk_sans_reg(X, Y, Y_pred_past, fk, loss) + reg(W, reg_gamma, T, reg_lambda)\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 3],\n",
    "        [4, 5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "Y = np.array([2.2, 8.7])\n",
    "Y_pred_past = np.array([1.0, 7.2])\n",
    "\n",
    "# Ceci n'est qu'un exemple d'une fonction d'un arbre pour faire fonctionner le test\n",
    "def f_test(X):\n",
    "    return np.sum(X/5)\n",
    "\n",
    "W = np.array([0.2, 0.3, -0.8, 0.7])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (1.5850000000000013, 1.03, 2.615000000000001)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Lk_sans_reg(X, Y, Y_pred_past, f_test, loss=SSE), reg(W, reg_gamma=0.1, T=4, reg_lambda=1), Lk(X, Y, Y_pred_past, f_test, W, loss=SSE, reg_gamma=0.1, T=4, reg_lambda=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.1 Quality Score\n",
    "\n",
    "C'est un score calculé lors de la construction d'un arbre XGBOOST, il sert à évaluer la division faite sur une caractéristique, et déterminer la meilleure division qui maximise ce score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I.3.1.1 Quality Score (Régression)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
