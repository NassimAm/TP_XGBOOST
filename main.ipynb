{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation et test du modèle XGBOOST\n",
    "---\n",
    "\n",
    "Dans ce notebook, on va découvrir les différents aspects du modèle XGBOOST. En premier lieu nous allons implémenter le modèle à zéro (from scratch) ensuite en deuxième partie nous allons directement utiliser le modèle depuis l'officielle bibliothèque ([xgboost](https://github.com/dmlc/xgboost)) et effectuer avec différents tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.3', '2.0.1')"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des modules nécessaires\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.__version__, pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Pour bien débuter, nous posons d'abord les premières notions qu'on va utiliser directement dans les prochaines sections.\n",
    "Tout au long du notebook, en parlant du dataset donné on note le nombre d'individus $n$ et le nombre de caractéristiques de chaque individu $m$. Ou plus explicitement le dataset ($D$) est donné comme il suit:\n",
    "$$D = \\{(x_i,y_i) \\text{ tq } x_i \\in R^m, y_i \\in R\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie servira à la compréhension des algorithmes utilisés dans un modèle XGBOOST en les implémentant de zéro pour les deux cas (Régression et Classification). Pour cela nous allons utiliser seulement la bibliothèque **numpy** qui est utile dans les calculs matriciels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Fonction de prédiction\n",
    "\n",
    "Pour un dataset donnée ($D$), la fonction de prédiction du modèle est la somme des $K + 1$ plus simples fonctions de prédiction. Les fonctions de prédiction ($f_k$) avec $k >= 1$ donne les résultats du parcours des arbres de régression construits lors de l'entrainement du modèle. Pour la fonction initiale ($f_0$), ce n'est qu'une valeur constante initiale déterminée au début de l'entraînement. On donne la fonction de prédiction du modèle donc comme il suit:\n",
    "$$\\hat{y_i} = \\phi(x_i) = f_0 + \\eta \\sum\\limits_{k = 1}\\limits^{K} f_k(x_i)$$\n",
    "Note: $\\eta$ représente le taux d'apprentissage (learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.6, 38.2])"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pred(X, f0, fs, eta=0.3):\n",
    "    return f0 + eta * np.sum(np.array([fk(X) for fk in fs]), axis=0)\n",
    "\n",
    "# Ceci n'est qu'un exemple d'une fonction d'un arbre pour faire fonctionner le test\n",
    "def f_test(X):\n",
    "    return np.sum(np.square(X) + X + 4, axis=1)\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [1, 3],\n",
    "        [4, 5]\n",
    "    ]\n",
    ")\n",
    "\n",
    "f0 = 3.4\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([16.6, 38.2])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "pred(X, f0, [f_test, f_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 Fonction du coût\n",
    "\n",
    "L'étape suivante serait de définir la fonction du coût (loss function) pour évaluer le modèle et le faire converger vers le point optimal. Or on a deux cas: la régression et la classification. Et donc on définira pour chacun des cas sa fonction de coût ensuite leurs dérivées pour pouvoir appliquer le [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) par la suite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.1 Fonction du coût (Régression)\n",
    "\n",
    "Pour la régression, nous allons utiliser la somme des erreurs carrées (sum of squares error, SSE). Elle est définie comme il suit\n",
    "$$SSE = l(y_i,\\hat{y_i}) = \\frac{1}{2} (y_i - \\hat{y_i})^2$$\n",
    "Note: la fraction $\\frac{1}{2}$ est ajoutée juste pour simplifier les calculs (notamment pour la dérivée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02 , 0.125, 0.125])"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SSE(Y, Y_pred):\n",
    "    return (1/2) * np.square(Y - Y_pred)\n",
    "\n",
    "Y = np.array([1.2, 3.5, 6.7])\n",
    "Y_pred = np.array([1.0, 4, 7.2])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([0.02 , 0.125, 0.125])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "SSE(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on calcule la dérivée de cette fonction de coût par rapport aux prédictions réalisées. Cela nous permettra de trouver la prédiction qui minimise la fonction coût. La dérivée est donc donnée comme il suit:\n",
    "$$\\frac{\\partial SSE}{\\partial \\hat{y_i}} = \\hat{y_i} - y_i$$\n",
    "Note: La valeur $y_i - \\hat{y_i}$ est appelée le résidu **$r_{ik}$** où $k$ représente le numéro de l'itération. Donc on a:\n",
    "$$r_{ik} = - \\frac{\\partial SSE}{\\partial \\hat{y_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2,  0.5,  0.5])"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dSSE(Y, Y_pred):\n",
    "    return Y_pred - Y\n",
    "\n",
    "Y = np.array([1.2, 3.5, 6.7])\n",
    "Y_pred = np.array([1.0, 4, 7.2])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([-0.2,  0.5,  0.5])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "dSSE(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2 Fonction du coût (Classification)\n",
    "\n",
    "Pour la classification (dans ce cas binaire), nous allons utiliser l'inverse du log de vraisemblance (negative log-likelihood). Elle est définie comme il suit\n",
    "$$NLL = l(y_i,p_i) = -\\text{ }(\\text{ }y_i \\log(p_i) + (1-y_i) \\log(1-p_i)\\text{ })$$\n",
    "Note: $p_i$ est la probabilité prédite par le modèle. Une valeur proche de 0 signifie que le modèle classe l'individu à la classe négative et une valeur proche de 1 signifie que le modèle classe l'individu à la classe positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35667494, 0.22314355, 0.22314355])"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NLL(Y, Y_pred):\n",
    "    return - (Y * np.log(Y_pred) + (1 - Y) * np.log(1 - Y_pred))\n",
    "\n",
    "Y = np.array([1, 0, 1])\n",
    "Y_pred = np.array([0.7, 0.2, 0.8])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([0.35667494, 0.22314355, 0.22314355])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "NLL(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on calcule la dérivée de cette fonction de coût par rapport au log de chance de gain (log of odds). Cela nous permettra de trouver la prédiction qui minimise la fonction coût. On rappele que la chance de gain (odds) d'un évènement de propababilité $p$ est définie par:\n",
    "$$odds = \\frac{p}{1 - p}$$\n",
    "$$\\log(odds) = \\log(\\frac{p}{1 - p})$$\n",
    "Dériver par rapport au $\\log(odds)$ est aussi un choix minutieux pour réduire et faciliter les calculs pour le modèle. Il est permis de faire ce choix car il y a une relation directe entre le $\\log(odds)$ et la probabilité $p$. En effet, On peut récupérer la probalité $p$ depuis $\\log(odds)$ ainsi:\n",
    "$$p = \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.708185057924486, 0.67)"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def p_to_log_odds(p):\n",
    "    return np.log(p/ (1-p))\n",
    "\n",
    "def log_odds_to_p(log_odds):\n",
    "    return np.exp(log_odds) / (1 + np.exp(log_odds))\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0.708185057924486, 0.67)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "p_to_log_odds(0.67), log_odds_to_p(0.708185057924486)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons maintenant la dérivée par rapport au $\\log(odds)$:\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -(\\text{ }y_i \\log(p_i) + (1-y_i) \\log(1-p_i)\\text{ })$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(p_i) + y_i \\log(1-p_i) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i (\\log(p_i) - \\log(1-p_i)) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(\\frac{p_i}{1-p_i}) - \\log(1-p_i)$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(odds) - \\log(1-p_i)$$\n",
    "On a:\n",
    "$$\\log(1-p_i) = \\log(1 - \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) = \\log(\\frac{1 + e^{\\log(odds)}}{1 + e^{\\log(odds)}} - \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}) = \\log(\\frac{1}{1 + e^{\\log(odds)}})$$\n",
    "$$\\log(1-p_i) = -\\log(1 + e^{\\log(odds)})$$\n",
    "Et donc:\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = \\frac{\\partial}{\\partial \\log(odds)} -y_i \\log(odds) + \\log(1 + e^{\\log(odds)})$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = -y_i + \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}}$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = p_i - y_i$$\n",
    "Note: La valeur $y_i - p_i$ est aussi appelée le résidu $r_{ik}$ où $k$ représente le numéro de l'itération. Donc on a:\n",
    "$$r_{ik} = - \\frac{\\partial NLL}{\\partial \\log(odds)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3,  0.2, -0.2])"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dNLL(Y, Y_pred):\n",
    "    return Y_pred - Y\n",
    "\n",
    "Y = np.array([1, 0, 1])\n",
    "Y_pred = np.array([0.7, 0.2, 0.8])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([-0.3,  0.2, -0.2])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "dNLL(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3 Constuction des arbres uniques de régression (Arbres XGBOOST)\n",
    "\n",
    "La fonction de prédiction du modèle XGBOOST dépend des arbres de régression construit lors de l'entrainement. Ces arbres sont des arbres spéciaux appelés des arbres uniques (ou arbres XGBOOST) qui servent à prédir des résidus ($r_{ik}$). En additionnant leurs résultats multipliés par un taux d'apprentissage et ajouté à la prédiction initiale, cela nous donne la prédiction finale du modèle (gradient tree boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.1 Fonction de coût de l'arbre\n",
    "\n",
    "Pendant la construction l'arbre, il faudrait minimiser une certaine fonction de coût (loss function). Son équation est donnée ci-dessous:\n",
    "$$L^{(k)} = \\sum\\limits_{i=1}\\limits^{n} l(y_i,\\hat{y_i}^{(k-1)}+f_k(x_i)) + \\Omega(f_k)$$\n",
    "$$L^{(k)} = \\sum\\limits_{i=1}\\limits^{n} l(y_i,\\hat{y_i}^{(k)}) + \\Omega(f_k)$$\n",
    "où\n",
    "- $y_i$ est la sortie réelle pour l'individu $i$\n",
    "- $\\hat{y_i}^{(k)}$ est la prédiction pour l'individu $i$ faite par le modèle à l'itération **$k$**\n",
    "- $f_k$ est la fonction qui donne pour tout individu $x_i$ le résultat de l'arbre construit à l'itération $k$\n",
    "- $l(y_i,\\hat{y_i})$ est la fonction de coût (SSE ou NLL dans notre cas)\n",
    "- $\\Omega(f_k)$ est un terme de régulation qui pénalise la complexité du modèle. Il est donné comme il suit:\n",
    "$$\\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\sum\\limits_{j=1}\\limits^{T} w_j^2$$\n",
    "où\n",
    "- $T$ est le nombre de feuilles dans l'arbre\n",
    "- $w_j$ est le poids de la feuille $j$ dans l'arbre (le calcul de ce poids sera détaillé juste après)\n",
    "- $\\gamma$ est un hyper-paramètre de régularisation, il définit la réduction minimale du coût pour réaliser une partition supplémentaire sur une feuille de l'arbre. Par défaut à 0, une plus grande valeur rend le modèle plus conservatif.\n",
    "- $\\lambda$ est un hyper-paramètre de régularisation L2 sur les poids des feuilles. Par défaut à 1, une plus grande valeur rend le modèle plus conservatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8449999999999989, 1.03, 2.874999999999999)"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction de coût sans le terme de régularisation\n",
    "def Lk_sans_reg(Y, Y_pred, loss=SSE):\n",
    "    return np.sum(loss(Y, Y_pred))\n",
    "\n",
    "# Terme de régularisation\n",
    "def reg(W, reg_lambda=1, reg_gamma=0, T=32):\n",
    "    return reg_gamma * T + (1/2) * reg_lambda * np.sum(np.square(W))\n",
    "\n",
    "# Fonction de coût complète\n",
    "def Lk(Y, Y_pred, W, loss=SSE, reg_lambda=1, reg_gamma=0, T=32):\n",
    "    return Lk_sans_reg(Y, Y_pred, loss) + reg(W, reg_lambda, reg_gamma, T)\n",
    "\n",
    "\n",
    "Y = np.array([2.2, 8.7])\n",
    "Y_pred = np.array([1.0, 7.2])\n",
    "\n",
    "W = np.array([0.2, 0.3, -0.8, 0.7])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (1.8449999999999989, 1.03, 2.874999999999999)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Lk_sans_reg(Y, Y_pred, loss=SSE), reg(W, reg_lambda=1, reg_gamma=0.1, T=4), Lk(Y, Y_pred, W, loss=SSE, reg_lambda=1, reg_gamma=0.1, T=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.2 Approximation de la fonction coût de l'arbre\n",
    "\n",
    "Maintenant que la fonction de coût est bien définie, nous pouvons trouver chaque sortie optimale $f_k(x_i)$ qui minimise le plus la fonction de coût. Celà nécessitera le calcul de dérivée de la fonction coût. Pour optimiser les calculs, le modèle XGBOOST approche le fonction coût par son polynôme de Taylor de second degré:\n",
    "$$l(y_i,\\hat{y_i}^{(k-1)}+f_k(x_i)) \\approx l(y_i,\\hat{y_i}^{(k-1)}) + \\frac{\\partial l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)}} f_k(x_i) + \\frac{1}{2} \\frac{\\partial^2 l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)2}} f_k(x_i)^2$$\n",
    "Et donc\n",
    "$$L^{(k)} \\approx \\sum\\limits_{i=1}\\limits^{n} [\\text{ } l(y_i,\\hat{y_i}^{(k-1)}) + \\frac{\\partial l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)}} f_k(x_i) + \\frac{1}{2} \\frac{\\partial^2 l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)2}} f_k(x_i)^2\\text{ }] + \\Omega(f_k)$$\n",
    "$$L^{(k)} \\approx \\sum\\limits_{i=1}\\limits^{n} [\\text{ } l(y_i,\\hat{y_i}^{(k-1)}) + g_i f_k(x_i) + \\frac{1}{2}h_i f_k(x_i)^2\\text{ }] + \\Omega(f_k)$$\n",
    "où\n",
    "- $g_i$ est la dérivée de la fonction coût par rapport à la prédiction précédente ($\\frac{\\partial l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)}}$)\n",
    "- $h_i$ est la seconde dérivée de la fonction coût par rapport à la prédiction précédente ($\\frac{\\partial^2 l(y_i,\\hat{y_i}^{(k-1)})}{\\partial \\hat{y_i}^{(k-1)2}}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.3 Quality Score et Poids des feuilles\n",
    "Définisons plus explicitement la fonction $f_k$, cette fonction nous donne le résultat renvoyé par l'arbre pour un individu avec ses caractéristiques. Ce résultat sera égal au poids d'une des feuilles ($w_j$) de la façon suivante:\n",
    "$$\\hat{y_i}^{(k)} = \\hat{y_i}^{(k-1)} + f_k(x_i)$$\n",
    "$$r_{ik} = y_i - \\hat{y_i}^{(k)}$$\n",
    "$$\\text{Si } r_{ik} \\in I_j \\text{ tel que } I_j \\text{ est l'ensemble des résidus se trouvant dans la feuille } j$$\n",
    "$$\\text{Donc } f_k(x_i) = w_j$$\n",
    "où $w_j$ est le poids de la feuille $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut considérer maintenant une nouvelle fonction de coût approximative pour l'arbre, où on peut omettre le terme $l(y_i,\\hat{y_i}^{(k-1)})$ qui ne représente que le coût de la précédente prédiction qui n'a pas besoin d'être minimisé dans la courante itération. Donc on a maintenant:\n",
    "\n",
    "$$\\tilde{L}^{(k)} = \\sum\\limits_{i=1}\\limits^{n} [\\text{ } g_i f_k(x_i) + \\frac{1}{2}h_i f_k(x_i)^2\\text{ }] + \\gamma T + \\frac{1}{2} \\lambda \\sum\\limits_{j=1}\\limits^{T} w_j^2$$\n",
    "$$\\tilde{L}^{(k)} = \\sum\\limits_{j=1}\\limits^{T} [\\text{ } (\\sum\\limits_{r_{ik} \\in I_j}g_i) w_j + \\frac{1}{2}(\\sum\\limits_{r_{ik} \\in I_j}h_i + \\lambda) w_j^2\\text{ }] + \\gamma T$$\n",
    "En résolvant l'équation $\\frac{\\partial \\tilde{L}^{(k)}}{\\partial w_j} = 0$ On obtient:\n",
    "$$w_j^* = -\\frac{\\sum\\limits_{r_{ik} \\in I_j}g_i}{\\sum\\limits_{r_{ik} \\in I_j}h_i + \\lambda}$$\n",
    "$$\\tilde{L}^{(k)}(w_j^*) = -\\frac{1}{2} \\sum\\limits_{j=1}\\limits^{T} \\frac{(\\sum\\limits_{r_{ik} \\in I_j}g_i)^2}{\\sum\\limits_{r_{ik} \\in I_j}h_i + \\lambda} + \\gamma T$$\n",
    "$$Q_j = \\frac{1}{2} \\frac{(\\sum\\limits_{r_{ik} \\in I_j}g_i)^2}{\\sum\\limits_{r_{ik} \\in I_j}h_i + \\lambda} - \\gamma$$\n",
    "où\n",
    "- $w_j^*$ est le poids optimal de la feuille\n",
    "- $\\tilde{L}^{(k)}(w_j^*)$ est le coût minimal pour l'arbre\n",
    "- $Q_j$ est appelée score de qualité ou similarité (Quality or Similarity Score). Puisque elle est inversement proportionelle au coût, le but est de la maximiser.\n",
    "- Remarque: le coût de l'arbre n'est que la somme des opposés des scores de qualité. C'est à dire: $\\tilde{L}^{(k)}(w_j^*) = \\sum\\limits_{j=1}\\limits^{T} - Q_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185.79512195121956"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcule le score de qualité (similiraté)\n",
    "def calculer_score(gs:np.ndarray, hs:np.ndarray, reg_lambda=1, reg_gamma=0):\n",
    "    if gs.size > 0 and hs.size > 0:\n",
    "        return ((1/2) * (np.sum(gs))**2 / (np.sum(hs) + reg_lambda)) - reg_gamma\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# On a maintenant toutes les informations pour créer un noeud de l'arbre\n",
    "# On définit une classe abstraite pour représenter un noeud en général\n",
    "# En réalité, cette classe représentera aussi toutes les feuilles (des noeuds qui ne sont ni NoeudCat ni NoeudNum)\n",
    "class Noeud:\n",
    "\n",
    "    nbr = 0\n",
    "    indent = '\\t'\n",
    "\n",
    "    def __init__(self, profondeur:int, num:int=None, val=None):\n",
    "        self.num = num # le numéro du caractéristique de division dans X\n",
    "        self.qs = 0 # le quality score pour le noeud\n",
    "        self.pr = profondeur # la profondeur du noeud\n",
    "        self.val = val # valeur du noeud\n",
    "        self.fils = {} # les fils du noeud\n",
    "        self.indices = np.array([]) # indices (i) des individus se classant dans le noeud\n",
    "        self.res = np.array([]) # les résidus trouvés dans le noeud (applicable que pour les feuilles)\n",
    "        self.gs = np.array([]) # les dérivés gi de premier ordre du noeud (applicable que pour les feuilles)\n",
    "        self.hs = np.array([]) # les dérivés hi de second ordre du noeud (applicable que pour les feuilles)\n",
    "        self.w = 0 # poids du noeud (applicable que pour les feuilles)\n",
    "\n",
    "    def ajuster(self, indices:np.ndarray, gs:np.ndarray, hs:np.ndarray, reg_lambda=1, reg_gamma=0):\n",
    "        self.gs = gs\n",
    "        self.hs = hs\n",
    "        self.res = - gs\n",
    "        self.indices = indices\n",
    "        self.calculer_poids(reg_lambda)\n",
    "        self.qs = calculer_score(gs, hs, reg_lambda, reg_gamma)\n",
    "    \n",
    "    def copier(self, other):\n",
    "        self.gs = other.gs\n",
    "        self.hs = other.hs\n",
    "        self.res = other.res\n",
    "        self.indices = other.indices\n",
    "        self.w = other.w\n",
    "        self.qs = other.qs\n",
    "\n",
    "    # Cette fonction sert à transformer le noeud à un string\n",
    "    def __str__(self):\n",
    "        tmp_indent = Noeud.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # Un noeud simple (non dérivé) n'affiche que son poids\n",
    "        return tmp_indent + f'W = {self.w}\\n'\n",
    "\n",
    "    # predire un échantillon\n",
    "    def predire(self, x: list):\n",
    "        # Un noeud simple (non dérivé) ne retourne que son poids\n",
    "        return self.w\n",
    "\n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self):\n",
    "        nid = 'N' + str(Noeud.nbr)\n",
    "        Noeud.nbr += 1\n",
    "        \n",
    "        # Un noeud simple (non dérivé) n'affiche que son poids\n",
    "        return nid, nid + '[label=\"' + str(self.w) + '\" shape=ellipse];\\n'\n",
    "\n",
    "    # calcule le poids d'un noeud\n",
    "    def calculer_poids(self, reg_lambda=1):\n",
    "        if self.gs.size > 0 and self.hs.size > 0:\n",
    "            self.w = - (np.sum(self.gs)) / (np.sum(self.hs) + reg_lambda)\n",
    "        else:\n",
    "            self.w = 0\n",
    "\n",
    "# On définit un noeud qu'on appelle NoeudCat pour gérer les données qualitatives, cette classe héritera de la classe Noeud\n",
    "class NoeudCat(Noeud):\n",
    "    # Cette fonction sert à transformer le noeud à un string\n",
    "    def __str__(self):\n",
    "        tmp_indent = Noeud.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # s'il n'y a pas de fils, le noeud est terminal ; on imprime la classe\n",
    "        if (len(self.fils)==0):\n",
    "            return tmp_indent + f'W = {self.w}\\n'\n",
    "        \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        res = tmp_indent + 'Si X[' + str(self.num) + '] est \"' + str(self.val) \n",
    "        res += '\" Alors\\n' + str(self.fils['g'])\n",
    "        res += tmp_indent + 'Sinon\\n' + str(self.fils['d'])\n",
    "        return res\n",
    "\n",
    "    # predire un échantillon\n",
    "    def predire(self, x: list[str]):\n",
    "        # Si le noeud est final, il rend son poids\n",
    "        if (len(self.fils)==0):\n",
    "            return self.w\n",
    "        \n",
    "        # sinon\n",
    "        if x[self.num] == self.val:\n",
    "            return self.fils['g'].predire(x)\n",
    "        return self.fils['d'].predire(x)\n",
    "\n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self):\n",
    "        nid = 'N' + str(Noeud.nbr)\n",
    "        Noeud.nbr += 1\n",
    "        \n",
    "        # Si le noeud est final, \n",
    "        if (len(self.fils)==0):\n",
    "            return nid, nid + '[label=\"' + str(self.w) + '\" shape=ellipse];\\n'\n",
    "        \n",
    "        # Sinon, \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        prefix = '] = '\n",
    "        res  = nid + '[label=\"X[' + str(self.num) + prefix + str(self.val) + '\\\\n'\n",
    "        res += 'QS = ' + str(self.qs) + '\"];\\n'\n",
    "        vid_G, code_G = self.fils['g'].graphviz()\n",
    "        vid_D, code_D = self.fils['d'].graphviz()\n",
    "        \n",
    "        res += code_G + code_D\n",
    "        res += nid + ' -> ' + vid_G + ' [label=\"Vrai\"];\\n'\n",
    "        res += nid + ' -> ' + vid_D + ' [label=\"Faux\"];\\n'\n",
    "        return nid, res\n",
    "    \n",
    "# On définit un noeud qu'on appelle NoeudNum pour gérer les données quantitatives, cette classe héritera de la classe Noeud\n",
    "class NoeudNum(Noeud):\n",
    "    # Cette fonction sert à transformer le noeud à un string\n",
    "    def __str__(self):\n",
    "\n",
    "        tmp_indent = self.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # s'il n'y a pas de fils, le noeud est terminal ; on imprime la classe\n",
    "        if (len(self.fils)==0):\n",
    "            return tmp_indent + f'W = {self.w}\\n'\n",
    "         \n",
    "        prefix = ' < '\n",
    "        suffix = ''\n",
    "        \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS SINON\n",
    "        res  = ''\n",
    "        res += tmp_indent + 'Si X[' + str(self.num) + ']' + prefix + str(self.val) + suffix \n",
    "        res += ' Alors\\n' + str(self.fils['g'])\n",
    "        res += tmp_indent + 'Sinon\\n' + str(self.fils['d'])\n",
    "        return res\n",
    "\n",
    "    # predire un échantillon\n",
    "    def predire(self, x: list):\n",
    "        # Si le noeud est final, il rend son poids\n",
    "        if (len(self.fils)==0):\n",
    "            return self.w\n",
    "        \n",
    "        # sinon\n",
    "        if x[self.num] < self.val:\n",
    "            return self.fils['g'].predire(x)\n",
    "        return self.fils['d'].predire(x)\n",
    "\n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self):\n",
    "        nid = 'N' + str(Noeud.nbr)\n",
    "        Noeud.nbr += 1\n",
    "        \n",
    "        # Si le noeud est final, \n",
    "        if (len(self.fils)==0):\n",
    "            return nid, nid + '[label=\"' + str(self.w) + '\" shape=ellipse];\\n'\n",
    "        \n",
    "        # Sinon, \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        prefix = '] < '\n",
    "        res = nid + '[label=\"X[' + str(self.num) + prefix + str(self.val) + '\\\\n'\n",
    "        res += 'QS = ' + str(self.qs) + '\"];\\n'\n",
    "        vid_G, code_G = self.fils['g'].graphviz()\n",
    "        vid_D, code_D = self.fils['d'].graphviz()\n",
    "        \n",
    "        res += code_G + code_D\n",
    "        res += nid + ' -> ' + vid_G + ' [label=\"Vrai\"];\\n'\n",
    "        res += nid + ' -> ' + vid_D + ' [label=\"Faux\"];\\n'\n",
    "        return nid, res\n",
    "    \n",
    "gs = np.array([10.7, -2.5, 4.3, 15.1])\n",
    "hs = np.array([0.25, 0.1, 0.3, 0.4])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# 185.79512195121956\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "calculer_score(gs, hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque nous allons avoir besoin des dérivés de second ordre (hessiennes), nous les calculons pour les deux fonctions de coût (loss functions) précédemment définies (SSE et NLL).\n",
    "$$\\frac{\\partial SSE}{\\partial \\hat{y_i}} = \\hat{y_i} - y_i \\Longleftrightarrow \\frac{\\partial^2 SSE}{\\partial \\hat{y_i}^2} = 1$$\n",
    "$$\\frac{\\partial NLL}{\\partial \\log(odds)} = -y_i + \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}} \\Longleftrightarrow \\frac{\\partial^2 NLL}{\\partial \\log(odds)^2} = \\frac{e^{\\log(odds)}}{(1 + e^{\\log(odds)})^2}$$\n",
    "$$\\Longleftrightarrow \\frac{\\partial^2 NLL}{\\partial \\log(odds)^2} = \\frac{e^{\\log(odds)}}{(1 + e^{\\log(odds)})} \\frac{1}{(1 + e^{\\log(odds)})} = p_i (1-p_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1.]), array([0.21, 0.16, 0.16]))"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hSSE(Y, Y_pred):\n",
    "    return np.ones(Y_pred.size)\n",
    "\n",
    "def hNLL(Y, Y_pred):\n",
    "    return Y_pred * (1-Y_pred)\n",
    "\n",
    "Y = np.array([1, 0, 1])\n",
    "Y_pred = np.array([0.7, 0.2, 0.8])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([1., 1., 1.]), array([0.21, 0.16, 0.16]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "hSSE(Y, Y_pred), hNLL(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.4 Choix des caractéristiques\n",
    "Pour choisir les caractéristiques ou les valeurs à comparer aux caractéristiques (quantiles) pour chaque noeud on calcule le gain en score. Ce gain est calculé comme il suit:\n",
    "$$gain = Q_g + Q_d - Q_r$$\n",
    "où\n",
    "- $Q_r$ représente le score de qualité au noeud courant\n",
    "- $Q_g$ représente le score de qualité au fils gauche\n",
    "- $Q_d$ représente le score de qualité au fils droit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, pour déterminer l'ensemble des quantiles pour les caractéristiques quantitatives (supposant que c'est l'utilisateur qui choisit le nombre des quantiles), XGBOOST utilise le principe des quantiles pondérés (Weighted Quantile Sketch), qui consiste à affecter à chaque individu un poids ($w_i$). Les intervalles créés par les quantiles choisis ont la même somme des poids de leurs individus.\n",
    "$$w_i^{(k)} = \\begin{cases}\n",
    "1 & \\text{pour la régression}\\\\\n",
    "p_i^{(k-1)} (1-p_i^{(k-1)}) & \\text{pour la classification}\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\\text{où } p_i^{(k-1)} \\text{ représente la probabilité pour l'individu }i\\text{ à l'itération précédente}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6, 3.2, 4.5, 7. ])"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fonction qui divise une colonne quantitative en utilisant le Weighted Quantile Sketch. Le résultat serait une liste de quantiles.\n",
    "# Remarque: la colonne Xj doit être ordonnée et W (le vecteur de poids des individus doit suivre le même ordre)\n",
    "# n: représente le nombre quantile à générer (dans la littérature XGBOOST génère par défaut 33 quantiles)\n",
    "def diviser_quantiles(Xj:np.ndarray, W:np.ndarray, n=33):\n",
    "    ind_sorted = Xj.argsort() # récupérer les indices ordonnés dans l'ordre croissant\n",
    "    r = [] # Résultats\n",
    "    d = np.sum(W)/n # Critère de division\n",
    "    W_cum = np.cumsum(W[ind_sorted])\n",
    "\n",
    "    mask = None\n",
    "    for i in range(n):\n",
    "        if i == n-1: # dans le cas où la dernière est exactement à la limite on la prend dans le dernier intervalle\n",
    "            mask = (W_cum >= i*d) & (W_cum <= (i+1)*d)\n",
    "        else:\n",
    "            mask = (W_cum >= i*d) & (W_cum < (i+1)*d)\n",
    "            \n",
    "        if len(Xj[ind_sorted][mask]) > 0:\n",
    "            r.append(np.max(Xj[ind_sorted][mask]))\n",
    "            \n",
    "    return np.array(r)\n",
    "\n",
    "Xj = np.array([3.2, 1.2, 1.6, 4.5, 5.5, 5.6, 7])\n",
    "W = np.array([0.2, 0.2, 0.3, 0.5, 0.5, 0.1, 0.3])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array([1.6, 3.2, 4.5, 7. ])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "diviser_quantiles(Xj, W, n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.5 Regrouper le tout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBTree:\n",
    "    # Initialiser l'arbre\n",
    "    def __init__(self, prev_pred, cls=False, max_depth=6, reg_lambda=1, reg_gamma=0):\n",
    "        self.cls = cls # est à Vrai, si c'est une classification, 0 sinon (régression)\n",
    "        if cls:\n",
    "            self.loss = NLL # fonction du coût\n",
    "            self.dloss = dNLL # dérivée de la fonction du coût\n",
    "            self.hloss = hNLL # seconde dérivée de la fonction du coût\n",
    "            self.prev_pred = p_to_log_odds(prev_pred) # log(odds)\n",
    "        else:\n",
    "            self.loss = SSE # fonction du coût\n",
    "            self.dloss = dSSE # dérivée de la fonction du coût\n",
    "            self.hloss = hSSE # seconde dérivée de la fonction du coût\n",
    "            self.prev_pred = prev_pred # précédente prédiction\n",
    "\n",
    "        self.reg_lambda = reg_lambda # hyper-paramètre de régularisation\n",
    "        self.reg_gamma = reg_gamma # hyper-paramètre de régularisation\n",
    "        self.W_feuilles = [] # poids des feuilles, cet attribut sera utile pour calculer le coût régularisé du modèle\n",
    "        self.max_depth = max_depth # hyper-paramètre qui limite la profondeur maximale de l'arbre\n",
    "        \n",
    "\n",
    "    # Pour un noeud donné et une caractéristique qualitative donnée choisir la modalité qui minimise le mieux le coût\n",
    "    def choisir_valeur_cat(self, Xj:np.ndarray, Y:np.ndarray, Y_pred: np.ndarray, qs_past: float):\n",
    "        s = np.unique(Xj)\n",
    "        max_gain = -1\n",
    "        gs_g_result = np.array([])\n",
    "        hs_g_result = np.array([])\n",
    "        gs_d_result = np.array([])\n",
    "        hs_d_result = np.array([])\n",
    "        mask_result = np.array([])\n",
    "        val = None\n",
    "        for sk in s:\n",
    "            mask = Xj == sk\n",
    "            gs_g = self.dloss(Y[mask], Y_pred[mask])\n",
    "            hs_g = self.hloss(Y[mask], Y_pred[mask])\n",
    "            gs_d = self.dloss(Y[~mask], Y_pred[~mask])\n",
    "            hs_d = self.hloss(Y[~mask], Y_pred[~mask])\n",
    "            qs_g = calculer_score(gs_g, hs_g, self.reg_lambda, self.reg_gamma)\n",
    "            qs_d = calculer_score(gs_d, hs_d, self.reg_lambda, self.reg_gamma)\n",
    "            gain_qs = qs_g + qs_d - qs_past\n",
    "            if gain_qs > max_gain:\n",
    "                gs_g_result = gs_g\n",
    "                hs_g_result = hs_g\n",
    "                gs_d_result = gs_d\n",
    "                hs_d_result = hs_d\n",
    "                mask_result = mask\n",
    "                max_gain = gain_qs\n",
    "                val = sk\n",
    "\n",
    "        return gs_g_result, hs_g_result, gs_d_result, hs_d_result, mask_result, max_gain, val\n",
    "    \n",
    "    # Pour un noeud donné et une caractéristique quantitative donnée choisir le quantile qui minimise le mieux le coût\n",
    "    def choisir_valeur_num(self, Xj:np.ndarray, Y:np.ndarray, Y_pred: np.ndarray, qs_past: float, W:np.ndarray, n=33):\n",
    "        nb_quantiles = min(len(Xj), n)\n",
    "        s = diviser_quantiles(Xj, W, nb_quantiles)\n",
    "        max_gain = -1\n",
    "        gs_g_result = np.array([])\n",
    "        hs_g_result = np.array([])\n",
    "        gs_d_result = np.array([])\n",
    "        hs_d_result = np.array([])\n",
    "        mask_result = np.array([])\n",
    "        val = None\n",
    "        for sk in s:\n",
    "            mask = Xj < sk\n",
    "            gs_g = self.dloss(Y[mask], Y_pred[mask])\n",
    "            hs_g = self.hloss(Y[mask], Y_pred[mask])\n",
    "            gs_d = self.dloss(Y[~mask], Y_pred[~mask])\n",
    "            hs_d = self.hloss(Y[~mask], Y_pred[~mask])\n",
    "            qs_g = calculer_score(gs_g, hs_g, self.reg_lambda, self.reg_gamma)\n",
    "            qs_d = calculer_score(gs_d, hs_d, self.reg_lambda, self.reg_gamma)\n",
    "            gain_qs = qs_g + qs_d - qs_past\n",
    "            if gain_qs > max_gain:\n",
    "                gs_g_result = gs_g\n",
    "                hs_g_result = hs_g\n",
    "                gs_d_result = gs_d\n",
    "                hs_d_result = hs_d\n",
    "                mask_result = mask\n",
    "                max_gain = gain_qs\n",
    "                val = sk\n",
    "\n",
    "        return gs_g_result, hs_g_result, gs_d_result, hs_d_result, mask_result, max_gain, val\n",
    "    \n",
    "    # Pour un noeud donné la caractéristique qui minimise le mieux le coût\n",
    "    def choisir_meilleur_noeud(self, r:Noeud, X:pd.DataFrame, Y:np.ndarray, Y_pred:np.ndarray, W:np.ndarray, n=33):\n",
    "        max_gain = -1\n",
    "        categorical = False\n",
    "        gs_g_result = np.array([])\n",
    "        hs_g_result = np.array([])\n",
    "        gs_d_result = np.array([])\n",
    "        hs_d_result = np.array([])\n",
    "        mask_result = np.array([])\n",
    "        max_j = 0\n",
    "        max_val = None\n",
    "        for j in range(X.shape[1]):\n",
    "            if np.issubdtype(X.iloc[:,j].dtype, np.number):\n",
    "                gs_g, hs_g, gs_d, hs_d, mask, gain_qs, val = self.choisir_valeur_num(X.iloc[:,j].values, Y, Y_pred, r.qs, W, n)\n",
    "                if gain_qs > max_gain:\n",
    "                    gs_g_result = gs_g\n",
    "                    hs_g_result = hs_g\n",
    "                    gs_d_result = gs_d\n",
    "                    hs_d_result = hs_d\n",
    "                    max_gain = gain_qs\n",
    "                    mask_result = mask\n",
    "                    max_j = j\n",
    "                    max_val = val\n",
    "                    categorical = False\n",
    "            else:\n",
    "                gs_g, hs_g, gs_d, hs_d, mask, gain_qs, val = self.choisir_valeur_cat(X.iloc[:,j].values, Y, Y_pred, r.qs)\n",
    "                if gain_qs > max_gain:\n",
    "                    gs_g_result = gs_g\n",
    "                    hs_g_result = hs_g\n",
    "                    gs_d_result = gs_d\n",
    "                    hs_d_result = hs_d\n",
    "                    max_gain = gain_qs\n",
    "                    mask_result = mask\n",
    "                    max_j = j\n",
    "                    max_val = val\n",
    "                    categorical = True\n",
    "\n",
    "        # Si le masque est homogène, c'est une feuille\n",
    "        if np.all(mask_result) or np.all(~mask_result):\n",
    "            self.T += 1 # On incrémente le nombre de feuilles\n",
    "            self.W_feuilles.append(r.w) # On sauvegarde son poids\n",
    "            return r\n",
    "\n",
    "        # Créer le noeud de décision\n",
    "        noeud: Noeud = None\n",
    "        if categorical:\n",
    "            noeud = NoeudCat(profondeur=r.pr, num=max_j, val=max_val)\n",
    "        else:\n",
    "            noeud = NoeudNum(profondeur=r.pr, num=max_j, val=max_val)\n",
    "        noeud.copier(r)\n",
    "        \n",
    "        # Création du noeud gauche\n",
    "        noeud_g = Noeud(profondeur=r.pr+1)\n",
    "        noeud_g.ajuster(r.indices[mask_result], gs_g_result, hs_g_result, self.reg_lambda, self.reg_gamma)\n",
    "\n",
    "        # Création du noeud droit\n",
    "        noeud_d = Noeud(profondeur=r.pr+1)\n",
    "        noeud_d.ajuster(r.indices[~mask_result], gs_d_result, hs_d_result, self.reg_lambda, self.reg_gamma)\n",
    "\n",
    "        # Lier les noeuds\n",
    "        noeud.fils = {\n",
    "            'g': noeud_g,\n",
    "            'd': noeud_d\n",
    "        }\n",
    "\n",
    "        return noeud\n",
    "\n",
    "    # Entrainer récursivement l'arbre\n",
    "    def entrainer_recursive(self, r:Noeud, X:pd.DataFrame, Y:np.ndarray, n:int=33):\n",
    "        if r.res.size <= 1: # S'arrêter s'il n'y a qu'un résidu dans le noeud (c'est une feuille)\n",
    "            self.T += 1 # On incrémente le nombre de feuilles\n",
    "            self.W_feuilles.append(r.w) # On sauvegarde son poids\n",
    "            return r\n",
    "        \n",
    "        Y_pred = self.prev_pred[r.indices] + np.array([r.predire(X.iloc[r.indices[i],:].values) for i in range(len(r.indices))])\n",
    "        if self.cls:\n",
    "            Y_pred = log_odds_to_p(Y_pred) # Si on entraine une classification, transformer les log(odds) en probabilité pour les prédiction\n",
    "            self.W[r.indices] = Y_pred * (1-Y_pred) # Mettre à jour les poids des individus si c'est une classification\n",
    "\n",
    "        noeud = self.choisir_meilleur_noeud(r, X.iloc[r.indices], Y[r.indices], Y_pred, self.W[r.indices], n) # Choisir la partition qui maximise le quality score\n",
    "\n",
    "        # Si le noeud n'est pas une feuille et on est pas arrivé encore à la profondeur maximale\n",
    "        if len(noeud.fils) > 0 and noeud.pr + 1 < self.max_depth:\n",
    "            noeud.fils['g'] = self.entrainer_recursive(noeud.fils['g'], X, Y, n) # entrainer le fils gauche\n",
    "            noeud.fils['d'] = self.entrainer_recursive(noeud.fils['d'], X, Y, n) # entrainer le fils droit\n",
    "\n",
    "        return noeud\n",
    "\n",
    "    # Fonction générale d'entraînement\n",
    "    def entrainer(self, X:pd.DataFrame, Y:np.ndarray, X_noms:list[str] = [], Y_nom:str = '', n:int=33):\n",
    "        self.W = np.ones(X.shape[0]) # Initialiser les poids des individus à 1\n",
    "        self.T = 0 # Nombre de feuilles\n",
    "        \n",
    "        # Créer le premier noeud de l'arbre\n",
    "        initial_pred = self.prev_pred\n",
    "        if self.cls:\n",
    "            initial_pred = log_odds_to_p(self.prev_pred)\n",
    "\n",
    "        self.arbre = Noeud(profondeur=0)\n",
    "        self.arbre.ajuster(indices=np.arange(X.shape[0]),gs=self.dloss(Y, initial_pred), hs=self.hloss(Y, initial_pred), \n",
    "                           reg_lambda=self.reg_lambda, reg_gamma=self.reg_gamma)\n",
    "        \n",
    "        # Entrainer récursivement l'arbre depuis le premier noeud\n",
    "        self.arbre = self.entrainer_recursive(self.arbre, X, Y, n)\n",
    "\n",
    "        # Sauvegarder une représentation textuelle de l'arbre\n",
    "        code = str(self.arbre)\n",
    "        if len(Y_nom) > 0: \n",
    "            code = code.replace('Y', Y_nom)\n",
    "        for i in range(len(X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', X_noms[i])\n",
    "        self.code   = code\n",
    "        self.X_noms = X_noms\n",
    "\n",
    "    # Fonction de prédiction de l'arbre (fk)\n",
    "    def predire(self, X:pd.DataFrame):\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]): \n",
    "            predictions.append(self.arbre.predire(X.iloc[i, :].values))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    # Fonction pour visualiser l'arbre sur GraphViz\n",
    "    def graphviz(self): \n",
    "        nid, code = self.arbre.graphviz()\n",
    "        res  = 'digraph Tree {\\n'\n",
    "        res += 'node [shape=box] ;'\n",
    "        for i in range(len(self.X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', self.X_noms[i])\n",
    "        res += code\n",
    "        res += '}'\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si humidite est \"haute\" Alors\n",
      "\tSi temps est \"ensoleile\" Alors\n",
      "\t\tW = -0.8624903265431408\n",
      "\tSinon\n",
      "\t\tSi temperature < 22.5 Alors\n",
      "\t\t\tW = -0.6280343548612316\n",
      "\t\tSinon\n",
      "\t\t\tW = 0.4846766221832071\n",
      "Sinon\n",
      "\tSi temperature < 19.4 Alors\n",
      "\t\tSi temps est \"ensoleile\" Alors\n",
      "\t\t\tW = 0.36019312966087014\n",
      "\t\tSinon\n",
      "\t\t\tW = -0.44145432310328875\n",
      "\tSinon\n",
      "\t\tW = 0.6242637766020209\n",
      "\n",
      "L'arbre a 6 feuilles\n"
     ]
    }
   ],
   "source": [
    "# Le dataset \"jouer\".\n",
    "\n",
    "# temps, temperature, humidite, vent\n",
    "X_jouer = pd.DataFrame([\n",
    "    ['ensoleile', 35.1 , 'haute'  , 'non'],\n",
    "    ['ensoleile', 34.7 , 'haute'  , 'oui'],\n",
    "    ['nuageux'  , 33.2 , 'haute'  , 'non'],\n",
    "    ['pluvieux' , 24.2  , 'haute'  , 'non'],\n",
    "    ['pluvieux' , 20.2, 'normale', 'non'],\n",
    "    ['pluvieux' , 18.3, 'normale', 'oui'],\n",
    "    ['nuageux'  , 19.4, 'normale', 'oui'],\n",
    "    ['ensoleile', 23.6  , 'haute'  , 'non'],\n",
    "    ['ensoleile', 17.7, 'normale', 'non'],\n",
    "    ['pluvieux' , 25.6  , 'normale', 'non'],\n",
    "    ['ensoleile', 23.3  , 'normale', 'oui'],\n",
    "    ['nuageux'  , 22.5  , 'haute'  , 'oui'],\n",
    "    ['nuageux'  , 34.4 , 'normale', 'non'],\n",
    "    ['pluvieux' , 21.5  , 'haute'  , 'oui']\n",
    "])\n",
    "\n",
    "Y_jouer = np.array([0, 0, 1, 1, 1, 0, 1, \n",
    "                    0, 1, 1, 1, 1, 1, 0])\n",
    "\n",
    "prev_pred = np.repeat(np.mean(Y_jouer), Y_jouer.size)\n",
    "tree = XGBTree(prev_pred, cls=True)\n",
    "tree.entrainer(X_jouer, Y_jouer, X_noms=[\"temps\", \"temperature\", \"humidite\", \"vent\"], Y_nom=\"jouer\", n=4)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Si humidite est \"haute\" Alors\n",
    "# \tSi temps est \"ensoleile\" Alors\n",
    "# \t\tW = -0.8624903265431408\n",
    "# \tSinon\n",
    "# \t\tSi temperature < 22.5 Alors\n",
    "# \t\t\tW = -0.6280343548612316\n",
    "# \t\tSinon\n",
    "# \t\t\tW = 0.4846766221832071\n",
    "# Sinon\n",
    "# \tSi temperature < 19.4 Alors\n",
    "# \t\tSi temps est \"ensoleile\" Alors\n",
    "# \t\t\tW = 0.36019312966087014\n",
    "# \t\tSinon\n",
    "# \t\t\tW = -0.44145432310328875\n",
    "# \tSinon\n",
    "# \t\tW = 0.6242637766020209\n",
    "#\n",
    "# L'arbre a 6 feuilles\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "print(tree.code)\n",
    "print(\"L'arbre a\", tree.T, \"feuilles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"794pt\" height=\"311pt\" viewBox=\"0.00 0.00 793.88 311.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 307)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-307 789.88,-307 789.88,4 -4,4\"/>\n",
       "<!-- N0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>N0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"479.49,-303 292.49,-303 292.49,-265 479.49,-265 479.49,-303\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.99\" y=\"-287.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">humidite = haute</text>\n",
       "<text text-anchor=\"middle\" x=\"385.99\" y=\"-272.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">QS = 5.264643753063955e-32</text>\n",
       "</g>\n",
       "<!-- N1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>N1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"372.99,-214 202.99,-214 202.99,-176 372.99,-176 372.99,-214\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.99\" y=\"-198.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">temps = ensoleile</text>\n",
       "<text text-anchor=\"middle\" x=\"287.99\" y=\"-183.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">QS = 0.4315068493150687</text>\n",
       "</g>\n",
       "<!-- N0&#45;&gt;N1 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>N0-&gt;N1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M365.69,-264.97C351.36,-252.26 331.99,-235.06 316.17,-221.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"318.07,-218.02 308.27,-214 313.43,-223.26 318.07,-218.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"354.99\" y=\"-235.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>N6</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"570.99,-214 400.99,-214 400.99,-176 570.99,-176 570.99,-214\"/>\n",
       "<text text-anchor=\"middle\" x=\"485.99\" y=\"-198.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">temperature &lt; 19.4</text>\n",
       "<text text-anchor=\"middle\" x=\"485.99\" y=\"-183.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">QS = 0.4315068493150683</text>\n",
       "</g>\n",
       "<!-- N0&#45;&gt;N6 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>N0-&gt;N6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M406.71,-264.97C421.46,-252.14 441.46,-234.74 457.68,-220.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"460.05,-223.21 465.3,-214 455.46,-217.93 460.05,-223.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"455.99\" y=\"-235.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "<!-- N2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>N2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"90.99\" cy=\"-106\" rx=\"90.98\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.99\" y=\"-102.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">-0.8624903265431408</text>\n",
       "</g>\n",
       "<!-- N1&#45;&gt;N2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>N1-&gt;N2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M247.17,-175.97C214.76,-161.66 169.48,-141.66 136.02,-126.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"137.04,-123.51 126.47,-122.67 134.21,-129.91 137.04,-123.51\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.99\" y=\"-146.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>N3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"375.99,-125 199.99,-125 199.99,-87 375.99,-87 375.99,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.99\" y=\"-109.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">temperature &lt; 22.5</text>\n",
       "<text text-anchor=\"middle\" x=\"287.99\" y=\"-94.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">QS = 0.24382141383660969</text>\n",
       "</g>\n",
       "<!-- N1&#45;&gt;N3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>N1-&gt;N3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287.99,-175.97C287.99,-164.19 287.99,-148.56 287.99,-135.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.49,-135 287.99,-125 284.49,-135 291.49,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"301.99\" y=\"-146.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "<!-- N4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>N4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"90.99\" cy=\"-18\" rx=\"90.98\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"90.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">-0.6280343548612316</text>\n",
       "</g>\n",
       "<!-- N3&#45;&gt;N4 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>N3-&gt;N4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M246.69,-86.97C214.45,-72.9 169.69,-53.36 136.43,-38.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"137.51,-35.49 126.94,-34.69 134.71,-41.9 137.51,-35.49\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.99\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>N5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"287.99\" cy=\"-18\" rx=\"87.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"287.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0.4846766221832071</text>\n",
       "</g>\n",
       "<!-- N3&#45;&gt;N5 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>N3-&gt;N5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M287.99,-86.76C287.99,-74.93 287.99,-59.32 287.99,-46.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.49,-46.04 287.99,-36.04 284.49,-46.04 291.49,-46.04\"/>\n",
       "<text text-anchor=\"middle\" x=\"301.99\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "<!-- N7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>N7</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"573.99,-125 397.99,-125 397.99,-87 573.99,-87 573.99,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"485.99\" y=\"-109.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">temps = ensoleile</text>\n",
       "<text text-anchor=\"middle\" x=\"485.99\" y=\"-94.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">QS = 0.10066222993222476</text>\n",
       "</g>\n",
       "<!-- N6&#45;&gt;N7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>N6-&gt;N7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M485.99,-175.97C485.99,-164.19 485.99,-148.56 485.99,-135.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"489.49,-135 485.99,-125 482.49,-135 489.49,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"497.99\" y=\"-146.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>N10</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"679.99\" cy=\"-106\" rx=\"87.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"679.99\" y=\"-102.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0.6242637766020209</text>\n",
       "</g>\n",
       "<!-- N6&#45;&gt;N10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>N6-&gt;N10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M526.19,-175.97C558.11,-161.66 602.7,-141.66 635.65,-126.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"637.36,-129.96 645.05,-122.67 634.49,-123.57 637.36,-129.96\"/>\n",
       "<text text-anchor=\"middle\" x=\"608.99\" y=\"-146.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "<!-- N8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>N8</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"485.99\" cy=\"-18\" rx=\"92.08\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"485.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0.36019312966087014</text>\n",
       "</g>\n",
       "<!-- N7&#45;&gt;N8 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>N7-&gt;N8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M485.99,-86.76C485.99,-74.93 485.99,-59.32 485.99,-46.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"489.49,-46.04 485.99,-36.04 482.49,-46.04 489.49,-46.04\"/>\n",
       "<text text-anchor=\"middle\" x=\"497.99\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>N9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"690.99\" cy=\"-18\" rx=\"94.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"690.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">-0.44145432310328875</text>\n",
       "</g>\n",
       "<!-- N7&#45;&gt;N9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>N7-&gt;N9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M528.97,-86.97C562.66,-72.83 609.5,-53.19 644.15,-38.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"645.71,-41.79 653.58,-34.69 643,-35.34 645.71,-41.79\"/>\n",
       "<text text-anchor=\"middle\" x=\"620.99\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation de l'arbre en graphe\n",
    "try:\n",
    "    from IPython.display import SVG\n",
    "    from graphviz import Source\n",
    "    from IPython.display import display\n",
    "    \n",
    "    graph = Source(tree.graphviz())\n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "except ImportError:\n",
    "    print('Il faut installer graphviz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4 Regrouper le modèle XGBOOST\n",
    "Maintenant qu'on a des arbres XGBOOST fonctionnels, on peut regrouper tout dans une classe qui représentera le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBOOST:\n",
    "    def __init__(self, cls=False, learning_rate=0.1, M=100, max_depth=6, reg_lambda=1, reg_gamma=0, n=33, proba=False, seuil=0.5):\n",
    "        self.cls = cls\n",
    "        self.learning_rate = learning_rate # taux d'apprentissage\n",
    "        self.M = M # nombre d'itérations\n",
    "        self.reg_lambda = reg_lambda # paramètre de régularisation (lambda)\n",
    "        self.reg_gamma = reg_gamma # paramètre de régularisation (gamma)\n",
    "        self.fks = [] # Les fonctions de prédictions des arbres\n",
    "        self.trees = [] # Les arbres xgboost\n",
    "        self.n = n # nombre de quantiles à extraire pendant le Weighted Quantile Sketch pour les données quantitative\n",
    "        self.couts = [] # coûts enregistrés à chaque itération\n",
    "        self.proba = proba # si vrai et le modèle est en mode classification, le modèle renvoie des probabilité sinon il renvoie des classes \n",
    "        self.seuil = seuil # seuil pour la classification (< seuil => 0, >= seuil => 1)\n",
    "        self.max_depth = max_depth # hyper-paramètre qui limite la profondeur maximale de l'arbre\n",
    "\n",
    "    # Fonction globale d'entrainement\n",
    "    def entrainer(self, X:pd.DataFrame, Y:np.ndarray, X_noms:list[str]=[], Y_nom:str=''):\n",
    "        self.initial_pred = np.repeat(np.mean(Y), Y.size) # Commencer par une prédiction naïve (la moyenne)\n",
    "        tmp_X_noms = X_noms\n",
    "        if len(tmp_X_noms) == 0:\n",
    "            tmp_X_noms = X.columns.values # Si les noms des colonnes ne sont pas donnés prendre ceux de la Dataframe\n",
    "\n",
    "        courante_pred = self.initial_pred\n",
    "        # Entrainer un XGBTree pour chaque itération\n",
    "        for i in range(self.M):\n",
    "            tree = XGBTree(courante_pred, cls=self.cls, max_depth=self.max_depth, reg_lambda=self.reg_lambda, reg_gamma=self.reg_gamma)\n",
    "            tree.entrainer(X, Y, X_noms=tmp_X_noms, Y_nom=Y_nom, n=self.n)\n",
    "\n",
    "            # Mettre à jour la prédiction précédente (avec un taux d'apprentissage)\n",
    "            if self.cls: # Pour le cas du classement les poids des feuilles sont des log(odds)\n",
    "                courante_pred = p_to_log_odds(courante_pred) + self.learning_rate * tree.predire(X)\n",
    "                courante_pred = log_odds_to_p(courante_pred)\n",
    "            else:\n",
    "                courante_pred = courante_pred + self.learning_rate * tree.predire(X)\n",
    "\n",
    "            # Sauvegarder les paramètres\n",
    "            self.fks.append(tree.predire) # Sauvegarder les fonctions de prédictions (fk)\n",
    "            self.trees.append(tree) # Sauvegarder les arbres (pour affichage et analyse)\n",
    "            self.couts.append(Lk(Y, courante_pred, tree.W_feuilles, tree.loss, self.reg_lambda, self.reg_gamma, tree.T)) # Sauvegarder les coûts\n",
    "    \n",
    "    # Fonction globale de prédiction\n",
    "    def predire(self, X:pd.DataFrame):\n",
    "        initial_pred = self.initial_pred # Commencer par la prédiction initiale\n",
    "        if self.cls: # Pour le cas du classement les poids des feuilles sont des log(odds)\n",
    "            initial_pred = p_to_log_odds(initial_pred)\n",
    "            log_result = pred(X, initial_pred, self.fks, eta=self.learning_rate) # fonction de prédiction\n",
    "            probas = log_odds_to_p(log_result)\n",
    "            \n",
    "            if self.proba:\n",
    "                return probas\n",
    "            else:\n",
    "                return (probas >= self.seuil).astype(int)\n",
    "        else:\n",
    "            return pred(X, initial_pred, self.fks, eta=self.learning_rate) # fonction de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        14\n",
      "   macro avg       1.00      1.00      1.00        14\n",
      "weighted avg       1.00      1.00      1.00        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test sur le dataset \"jouer\"\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = XGBOOST(cls=True, learning_rate=0.4, M=100, n=4)\n",
    "model.entrainer(X_jouer, Y_jouer, X_noms=[\"temps\", \"temperature\", \"humidite\", \"vent\"], Y_nom=\"jouer\")\n",
    "\n",
    "print(classification_report(Y_jouer, model.predire(X_jouer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage de la fonction coût globale du modèle au fil des itérations\n",
    "plt.plot(model.couts)\n",
    "plt.xlabel(\"Itérations\")\n",
    "plt.ylabel(\"Coût\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
